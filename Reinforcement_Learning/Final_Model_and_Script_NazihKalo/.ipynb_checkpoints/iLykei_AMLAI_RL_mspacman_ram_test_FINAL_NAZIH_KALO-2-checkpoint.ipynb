{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "\n",
    "# Advanced Machine Learning and Artificial Intelligence (MScA 32017)\n",
    "\n",
    "# Pac-Man Competition for Human-Machine Teams \n",
    "\n",
    "### Y.Balasanov, M. Tselishchev, &copy; iLykei 2018\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import os \n",
    "from keras.models import Sequential, clone_model, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, InputLayer, Lambda, Input\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import VarianceScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.9\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)\n",
    "#os.__version__\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model (which was previously saved by `model.save()`-method) for online network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import initializers, regularizers, activations, constraints\n",
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class NoisyNetDense(Layer):\n",
    "    \"\"\"\n",
    "    A modified fully-connected layer that injects noise into the parameter distribution\n",
    "    before each prediction. This randomness forces the agent to explore - at least\n",
    "    until it can adjust its parameters to learn around it.\n",
    "    To use: replace Dense layers (like the classifier at the end of a DQN model)\n",
    "    with NoisyNetDense layers and set your policy to GreedyQ.\n",
    "    See examples/noisynet_pdd_dqn_atari.py\n",
    "    Reference: https://arxiv.org/abs/1706.10295\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                units,\n",
    "                activation=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None,\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                mu_initializer=None,\n",
    "                sigma_initializer=None,\n",
    "                **kwargs):\n",
    "\n",
    "        super(NoisyNetDense, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint) if kernel_constraint is not None else None\n",
    "        self.bias_constraint = constraints.get(bias_constraint) if kernel_constraint is not None else None\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)if kernel_constraint is not None else None\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer) if kernel_constraint is not None else None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[-1]\n",
    "\n",
    "        #See section 3.2 of Fortunato et al.\n",
    "        sqr_inputs = self.input_dim**(1/2)\n",
    "        self.sigma_initializer = initializers.Constant(value=.5/sqr_inputs)\n",
    "        self.mu_initializer = initializers.RandomUniform(minval=(-1/sqr_inputs), maxval=(1/sqr_inputs))\n",
    "\n",
    "\n",
    "        self.mu_weight = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                        initializer=self.mu_initializer,\n",
    "                                        name='mu_weights',\n",
    "                                        constraint=self.kernel_constraint,\n",
    "                                        regularizer=self.kernel_regularizer)\n",
    "\n",
    "        self.sigma_weight = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                        initializer=self.sigma_initializer,\n",
    "                                        name='sigma_weights',\n",
    "                                        constraint=self.kernel_constraint,\n",
    "                                        regularizer=self.kernel_regularizer)\n",
    "\n",
    "        self.mu_bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.mu_initializer,\n",
    "                                        name='mu_bias',\n",
    "                                        constraint=self.bias_constraint,\n",
    "                                        regularizer=self.bias_regularizer)\n",
    "\n",
    "        self.sigma_bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.sigma_initializer,\n",
    "                                        name='sigma_bias',\n",
    "                                        constraint=self.bias_constraint,\n",
    "                                        regularizer=self.bias_regularizer)\n",
    "\n",
    "        super(NoisyNetDense, self).build(input_shape=input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        #sample from noise distribution\n",
    "        e_i = K.random_normal((self.input_dim, self.units))\n",
    "        e_j = K.random_normal((self.units,))\n",
    "\n",
    "        #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al.\n",
    "        eW = K.sign(e_i)*(K.sqrt(K.abs(e_i))) * K.sign(e_j)*(K.sqrt(K.abs(e_j)))\n",
    "        eB = K.sign(e_j)*(K.abs(e_j)**(1/2))\n",
    "\n",
    "        #See section 3 of Fortunato et al.\n",
    "        noise_injected_weights = K.dot(x, self.mu_weight + (self.sigma_weight * eW))\n",
    "        noise_injected_bias = self.mu_bias + (self.sigma_bias * eB)\n",
    "        output = K.bias_add(noise_injected_weights, noise_injected_bias)\n",
    "        if self.activation != None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'mu_initializer': initializers.serialize(self.mu_initializer),\n",
    "            'sigma_initializer': initializers.serialize(self.sigma_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(NoisyNetDense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, dense_layers, dense_units):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for i in range(dense_layers):\n",
    "        model.add(Dense(units=dense_units, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.3.0-el7-x86_64/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "input_shape = env.reset().shape #(128,)\n",
    "nb_actions = env.action_space.n  # 9\n",
    "dense_layers = 2\n",
    "dense_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon, n_outputs):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_outputs)  # random action\n",
    "    else:\n",
    "        return np.argmax(q_values)          # q-optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard DQN model architecture, but swapping the Dense classifier layers for the rl.layers.NoisyNetDense version.\n",
    "def create_noisy_model(input_shape = input_shape, nb_actions = nb_actions, dense_layers = 4, dense_units = 256):\n",
    "    model = Sequential()\n",
    "    #model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    for i in range(dense_layers):\n",
    "        model.add(Dense(units=dense_units, activation='relu'))\n",
    "    model.add(NoisyNetDense(units=dense_units, activation='relu'))\n",
    "    model.add(NoisyNetDense(nb_actions, activation='linear'))\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $\\varepsilon$-greedy strategy (using small $\\varepsilon$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "\n",
    "Define a function to evalutate the trained network. \n",
    "Note that we still using $\\varepsilon$-greedy strategy here to prevent an agent from getting stuck. \n",
    "`test_dqn` returns a list with scores for specific number of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(n_games, model, nb_actions=9, skip_start=90, eps=0.05, render=False, sleep_time=0.01):\n",
    "    env = gym.make(\"MsPacman-ram-v0\")\n",
    "    scores = []\n",
    "    for i in range(n_games):\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        for skip in range(skip_start):  # skip the start of each game (it's just freezing time before game starts)\n",
    "            obs, reward, done, info = env.step(0)\n",
    "            score += reward\n",
    "        while not done:\n",
    "            state = obs\n",
    "            q_values = model.predict(np.array([state]))[0]\n",
    "            action = epsilon_greedy(q_values, eps, nb_actions)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep_time)\n",
    "                if done:\n",
    "                    time.sleep(1)\n",
    "        scores.append(score)\n",
    "        # print('{}/{}: {}'.format(i+1, n_games, score))\n",
    "        env.close()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = env.reset().shape\n",
    "nb_actions = env.action_space.n \n",
    "fourlayer_reducedLR  = create_noisy_model(input_shape = input_shape, \n",
    "                                          nb_actions = nb_actions, \n",
    "                                          dense_layers = 2, \n",
    "                                          dense_units = 256)\n",
    "fourlayer_reducedLR.load_weights('models/6Layer_256_D0.9999_Upd4_reducedLR/2730.00med_2523.20avg__640.00min__2020.03.12.16.46.19.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = Sequential()\n",
    "updated_model.add(InputLayer(input_shape))\n",
    "updated_model.add(Lambda(lambda x: x/255, name = 'normalizer'))\n",
    "updated_model.add(fourlayer_reducedLR.layers[0])\n",
    "updated_model.add(fourlayer_reducedLR.layers[1])\n",
    "updated_model.add(fourlayer_reducedLR.layers[2])\n",
    "updated_model.add(fourlayer_reducedLR.layers[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalizer (Lambda)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "noisy_net_dense_1 (NoisyNetD (None, 256)               131584    \n",
      "_________________________________________________________________\n",
      "noisy_net_dense_2 (NoisyNetD (None, 9)                 4626      \n",
      "=================================================================\n",
      "Total params: 235,026\n",
      "Trainable params: 235,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "updated_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting scores\n",
    "\n",
    "Run 100 games without rendering and collect necessary statistics for final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median score:  2725.0\n",
      "\n",
      "Mean score:  2629.6\n",
      "\n",
      "Max score:  3950.0\n",
      "\n",
      "Fifth percentile:  3351.0\n",
      "\n",
      "Percentiles:\n",
      "[430.0, 2335.0, 2725.0, 2930.0, 3950.0]\n"
     ]
    }
   ],
   "source": [
    "n_games = 100\n",
    "eps = 0\n",
    "nb_actions = 9\n",
    "render = False \n",
    "\n",
    "scores = test_dqn(n_games = n_games, \n",
    "                  model = updated_model, \n",
    "                  nb_actions = nb_actions, \n",
    "                  eps=eps, render=render)\n",
    "\n",
    "print('\\nMedian score: ', np.median(scores))\n",
    "print('\\nMean score: ', np.mean(scores))\n",
    "print('\\nMax score: ', np.max(scores))\n",
    "print('\\nFifth percentile: ',np.percentile(scores, 95))\n",
    "print('\\nPercentiles:')\n",
    "print([ np.percentile(scores, p) for p in [0, 25, 50, 75, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median score:  2600.0\n",
      "\n",
      "Mean score:  2580.1\n",
      "\n",
      "Max score:  3850.0\n",
      "\n",
      "Fifth percentile:  3321.5\n",
      "\n",
      "Percentiles:\n",
      "[490.0, 2375.0, 2600.0, 2827.5, 3850.0]\n"
     ]
    }
   ],
   "source": [
    "n_games = 100\n",
    "eps = 0\n",
    "nb_actions = 9\n",
    "render = False \n",
    "\n",
    "scores = test_dqn(n_games = n_games, \n",
    "                  model = updated_model, \n",
    "                  nb_actions = nb_actions, \n",
    "                  eps=eps, render=render)\n",
    "\n",
    "print('\\nMedian score: ', np.median(scores))\n",
    "print('\\nMean score: ', np.mean(scores))\n",
    "print('\\nMax score: ', np.max(scores))\n",
    "print('\\nFifth percentile: ',np.percentile(scores, 95))\n",
    "print('\\nPercentiles:')\n",
    "print([ np.percentile(scores, p) for p in [0, 25, 50, 75, 100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "\n",
    "Play 3 more times with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ngames = 5\n",
    "eps = 0.05\n",
    "render = True\n",
    "\n",
    "scores = test_dqn(ngames, online_network, eps=eps, render=render)\n",
    "\n",
    "print('\\nMean score: ', np.mean(scores))\n",
    "print('\\nMax score: ', np.max(scores))\n",
    "print('\\nPercentiles:')\n",
    "print([ np.percentile(scores, p) for p in [0, 25, 50, 75, 100] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
