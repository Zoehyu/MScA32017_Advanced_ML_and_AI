{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning and Artificial Intelligence (MScA 32017)\n",
    "\n",
    "# Project: Satellite Image Segmentation Using U-Net\n",
    "\n",
    "### Isabelle Hu, Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load code/train_unet.py\n",
    "from unet_model import *\n",
    "from gen_patches import *\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    min = img.min()\n",
    "    max = img.max()\n",
    "    x = 2.0 * (img - min) / (max - min) - 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "N_BANDS = 8\n",
    "N_CLASSES = 5  # buildings, roads, trees, crops and water\n",
    "CLASS_WEIGHTS = [0.2, 0.3, 0.1, 0.1, 0.3]\n",
    "N_EPOCHS = 100\n",
    "UPCONV = True\n",
    "PATCH_SZ = 160   # should divide by 16\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_SZ = 4000  # train size\n",
    "VAL_SZ = 1000    # validation size\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return unet_model(N_CLASSES, PATCH_SZ, n_channels=N_BANDS, upconv=UPCONV, class_weights=CLASS_WEIGHTS)\n",
    "\n",
    "\n",
    "weights_path = 'weights'\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "weights_path += '/unet_weights.hdf5'\n",
    "\n",
    "trainIds = [str(i).zfill(2) for i in range(1, 25)]  # all availiable ids: from \"01\" to \"24\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_DICT_TRAIN = dict()\n",
    "    Y_DICT_TRAIN = dict()\n",
    "    X_DICT_VALIDATION = dict()\n",
    "    Y_DICT_VALIDATION = dict()\n",
    "\n",
    "    print('Reading images')\n",
    "    for img_id in trainIds:\n",
    "        img_m = normalize(tiff.imread('./data/mband/{}.tif'.format(img_id)).transpose([1, 2, 0]))\n",
    "        mask = tiff.imread('./data/gt_mband/{}.tif'.format(img_id)).transpose([1, 2, 0]) / 255\n",
    "        train_xsz = int(3/4 * img_m.shape[0])  # use 75% of image as train and 25% for validation\n",
    "        X_DICT_TRAIN[img_id] = img_m[:train_xsz, :, :]\n",
    "        Y_DICT_TRAIN[img_id] = mask[:train_xsz, :, :]\n",
    "        X_DICT_VALIDATION[img_id] = img_m[train_xsz:, :, :]\n",
    "        Y_DICT_VALIDATION[img_id] = mask[train_xsz:, :, :]\n",
    "        print(img_id + ' read')\n",
    "    print('Images were read')\n",
    "\n",
    "    def train_net():\n",
    "        print(\"start train net\")\n",
    "        x_train, y_train = get_patches(X_DICT_TRAIN, Y_DICT_TRAIN, n_patches=TRAIN_SZ, sz=PATCH_SZ)\n",
    "        x_val, y_val = get_patches(X_DICT_VALIDATION, Y_DICT_VALIDATION, n_patches=VAL_SZ, sz=PATCH_SZ)\n",
    "        model = get_model()\n",
    "        if os.path.isfile(weights_path):\n",
    "            model.load_weights(weights_path)\n",
    "        model_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True)\n",
    "        csv_logger = CSVLogger('log_unet.csv', append=True, separator=';')\n",
    "        tensorboard = TensorBoard(log_dir='./tensorboard_unet/', write_graph=True, write_images=True)\n",
    "        model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS,\n",
    "                  verbose=2, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, csv_logger, tensorboard],\n",
    "                  validation_data=(x_val, y_val))\n",
    "        return model\n",
    "\n",
    "    train_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load code/predict.py\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "\n",
    "from train_unet import weights_path, get_model, normalize, PATCH_SZ, N_CLASSES\n",
    "\n",
    "\n",
    "def predict(x, model, patch_sz=160, n_classes=5):\n",
    "    img_height = x.shape[0]\n",
    "    img_width = x.shape[1]\n",
    "    n_channels = x.shape[2]\n",
    "    # make extended img so that it contains integer number of patches\n",
    "    npatches_vertical = math.ceil(img_height / patch_sz)\n",
    "    npatches_horizontal = math.ceil(img_width / patch_sz)\n",
    "    extended_height = patch_sz * npatches_vertical\n",
    "    extended_width = patch_sz * npatches_horizontal\n",
    "    ext_x = np.zeros(shape=(extended_height, extended_width, n_channels), dtype=np.float32)\n",
    "    # fill extended image with mirrors:\n",
    "    ext_x[:img_height, :img_width, :] = x\n",
    "    for i in range(img_height, extended_height):\n",
    "        ext_x[i, :, :] = ext_x[2 * img_height - i - 1, :, :]\n",
    "    for j in range(img_width, extended_width):\n",
    "        ext_x[:, j, :] = ext_x[:, 2 * img_width - j - 1, :]\n",
    "\n",
    "    # now we assemble all patches in one array\n",
    "    patches_list = []\n",
    "    for i in range(0, npatches_vertical):\n",
    "        for j in range(0, npatches_horizontal):\n",
    "            x0, x1 = i * patch_sz, (i + 1) * patch_sz\n",
    "            y0, y1 = j * patch_sz, (j + 1) * patch_sz\n",
    "            patches_list.append(ext_x[x0:x1, y0:y1, :])\n",
    "    # model.predict() needs numpy array rather than a list\n",
    "    patches_array = np.asarray(patches_list)\n",
    "    # predictions:\n",
    "    patches_predict = model.predict(patches_array, batch_size=4)\n",
    "    prediction = np.zeros(shape=(extended_height, extended_width, n_classes), dtype=np.float32)\n",
    "    for k in range(patches_predict.shape[0]):\n",
    "        i = k // npatches_horizontal\n",
    "        j = k % npatches_vertical\n",
    "        x0, x1 = i * patch_sz, (i + 1) * patch_sz\n",
    "        y0, y1 = j * patch_sz, (j + 1) * patch_sz\n",
    "        prediction[x0:x1, y0:y1, :] = patches_predict[k, :, :, :]\n",
    "    return prediction[:img_height, :img_width, :]\n",
    "\n",
    "\n",
    "def picture_from_mask(mask, threshold=0):\n",
    "    colors = {\n",
    "        0: [150, 150, 150],  # Buildings\n",
    "        1: [223, 194, 125],  # Roads & Tracks\n",
    "        2: [27, 120, 55],    # Trees\n",
    "        3: [166, 219, 160],  # Crops\n",
    "        4: [116, 173, 209]   # Water\n",
    "    }\n",
    "    z_order = {\n",
    "        1: 3,\n",
    "        2: 4,\n",
    "        3: 0,\n",
    "        4: 1,\n",
    "        5: 2\n",
    "    }\n",
    "    pict = 255*np.ones(shape=(3, mask.shape[1], mask.shape[2]), dtype=np.uint8)\n",
    "    for i in range(1, 6):\n",
    "        cl = z_order[i]\n",
    "        for ch in range(3):\n",
    "            pict[ch,:,:][mask[cl,:,:] > threshold] = colors[cl][ch]\n",
    "    return pict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = get_model()\n",
    "    model.load_weights(weights_path)\n",
    "    test_id = 'test'\n",
    "    img = normalize(tiff.imread('data/mband/{}.tif'.format(test_id)).transpose([1,2,0]))   # make channels last\n",
    "    mask = predict(img, model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])  # make channels first\n",
    "    map = picture_from_mask(mask, 0.5)\n",
    "\n",
    "    tiff.imsave('result.tif', (255*mask).astype('uint8'))\n",
    "    tiff.imsave('map.tif', map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
